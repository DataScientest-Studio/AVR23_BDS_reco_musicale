{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20b8cd1-63e1-48d1-8870-c98fcee7b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8194fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576b5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.user_playlist_utils import user_filter, summarise_listening_history\n",
    "from ipynb.fs.full.evaluation_workflow import split_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942111bb-2d23-49c1-9e98-a79122be56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listenings_history_partition(listenings_history_feats, listenings_history, max_clusters = 15, random_state = None):\n",
    "    s_scores = []\n",
    "    labels = []\n",
    "    centroids = []\n",
    "    n_clusters_range = np.arange(2, min(max_clusters, len(listenings_history_feats) - 1))\n",
    "    \n",
    "    # Search the optimal number of clusters\n",
    "    for i in n_clusters_range:\n",
    "        clf = KMeans(n_clusters = i, n_init = \"auto\", random_state = random_state)\n",
    "        clf.fit(listenings_history_feats, sample_weight= listenings_history.listening_count)\n",
    "        centroids.append(clf.cluster_centers_)\n",
    "        labels.append(clf.labels_)\n",
    "        score = silhouette_score(listenings_history_feats, labels[i-2] , metric='euclidean') \n",
    "        s_scores.append(score)\n",
    "    \n",
    "    # Define the optimal number of clusters from the silhouette score\n",
    "    i_clusters_opt = s_scores.index(max(s_scores))\n",
    "    n_cluster_opt = i_clusters_opt +2 \n",
    "\n",
    "    # Return the corresponding partition, \n",
    "    return labels[i_clusters_opt], centroids[i_clusters_opt], s_scores[i_clusters_opt], n_cluster_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0274c7d-a8eb-4531-bf6d-cee8732b3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kmeans_based_ranking(listenings_history, X,  weighted = False, n_clusters=\"Default\",  random_state = None):  \n",
    "    \"\"\"\n",
    "    Define the centroid(s) representing the average audio features of the tracks listened by a user.\n",
    "    Estimate the affinity of the user for tracks that he didn't listened based on their distance with this (these) centroid(s).\n",
    "    Return the rank of the unlistened tracks based on this afinity.\n",
    "    \n",
    "    Parameters:\n",
    "        listenings_history (pandas.Dataframe): a dataframe whose columns are\n",
    "            - user : a unique id of the user\n",
    "            \n",
    "            - track_id : a unique id for a track\n",
    "            \n",
    "            - listening_count: the number of times the user has listened to the track\n",
    "            \n",
    "            ... : other columns corresponding to track's features and/or the user's features and/or element of context of the interaction user/track.\n",
    "        \n",
    "        X (pandas.Dataframe): a dataframe corresponding to the audio_features of the tracks.\n",
    "        \n",
    "        weighted (boolean, default = False): determines whether the centroid calculation is weighted by the listens number of the tracks.    \n",
    "         \n",
    "        n_clusters (int or 'auto'): define the number of centroids to form. If 'auto', it is determine as the arg max of the silhouette score.\n",
    "        \n",
    "        \n",
    "        random_state (int, default=None): pass an int for reproducible output across multiple function calls.\n",
    "        \n",
    "    Return :\n",
    "       R : the ranks of each tracks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the features of the songs listened by the user\n",
    "    listenings_history_feats = X.loc[listenings_history.track_id]      \n",
    "    \n",
    "    \n",
    "    _, _ , _, n_clust_opt =  get_listenings_history_partition(listenings_history_feats, listenings_history, random_state = random_state)\n",
    "    # If a weighting of each tracks is desired :\n",
    "    if weighted:\n",
    "        # Calculation of the weight of each track according to its number of listens\n",
    "        w = listenings_history.listening_count / listenings_history.listening_count.sum()\n",
    "        w.index = listenings_history_feats.index \n",
    "        # Tracks weighting\n",
    "        listenings_history_feats = listenings_history_feats.apply(lambda x: x*w)\n",
    "    \n",
    "    # Compute the user tracks centroids\n",
    "    if n_clusters == 'auto':\n",
    "        labels, centroids, _ ,n_clust_opt = get_listenings_history_partition(listenings_history_feats, listenings_history, random_state = random_state)\n",
    "    elif n_clusters == 1:\n",
    "        labels = np.zeros(len(listenings_history))\n",
    "        centroids = np.array(listenings_history_feats.apply('mean'), ndmin = 2)\n",
    "    elif n_clusters == 'Default':\n",
    "        clf = KMeans(n_clusters = n_clust_opt, n_init = 'auto', random_state = random_state)\n",
    "        clf.fit(listenings_history_feats, sample_weight=listenings_history.listening_count)\n",
    "        centroids = clf.cluster_centers_\n",
    "        labels = clf.labels_\n",
    "    \n",
    "    # Define the number of neighbors to find according to the clusters size\n",
    "    # cluster_size = pd.Series(labels).value_counts()\n",
    "    # nb_tracks_by_clusters = [round(n_neighbors * v) for v in cluster_size / sum(cluster_size)]\n",
    "    # if not sum(nb_tracks_by_clusters) == n_neighbors:\n",
    "    #     nb_tracks_by_clusters[-1] = n_neighbors - sum(nb_tracks_by_clusters[:-1])\n",
    "        \n",
    "    # Compute the distance between the HIDDEN tracks acoustic characteristics (X.index.difference(listenings_history_feats.index)) and the centroids calculated on the APPARENT playlist \n",
    "    D = pd.DataFrame(distance_matrix(X.loc[X.index.difference(listenings_history_feats.index)], centroids), index = X.index.difference(listenings_history_feats.index))           \n",
    "\n",
    "#     # Get the ranks of the tracks as a function of to its distance with each centroid\n",
    "    R = D.rank(axis = 0)\n",
    "\n",
    "#     # Get the n_neighbors unique recommended tracks\n",
    "#     recommended_tracks = []\n",
    "#     for i, n in enumerate(nb_tracks_by_clusters):\n",
    "#         tracks = [t for t in list(R.iloc[:,i].sort_values().index) if not t in recommended_tracks]\n",
    "#         recommended_tracks = recommended_tracks + tracks[:n]\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_artist_filter(initial_ranking, apparent_history: pd.DataFrame):\n",
    "    \n",
    "    initial_ranking.insert(0, 'artist_name', initial_ranking['track_id'].str.split(' - ').str[1])\n",
    "    initial_ranking['min_centroids'] =  np.nan\n",
    "    initial_ranking['artist_rank'] =  np.nan\n",
    "    # get the artists and sort them by proportion in df_user_apparent  \n",
    "    user_artists = (apparent_history.groupby('artist_name')['track_id'].count()/len(apparent_history)).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "    # create a blank df with same columns as user_rank\n",
    "    # calculate the minimum between centroids for each track of artists identified in df_user_apparent\n",
    "    # sort them by this minimum distance\n",
    "    # attribute them a new rank from 1 to len(track_ids) of each artist identifies in order of their importance in df_user_apparent \n",
    "    user_rank_artist = initial_ranking.drop(index = initial_ranking.index, axis=0)\n",
    "    for i, artist in enumerate(user_artists.index):\n",
    "        user_temp = initial_ranking[initial_ranking['artist_name'] == user_artists.index[i]]\n",
    "        user_temp['min_centroids'] = user_temp.iloc[:, 2:-2].min(axis=1)\n",
    "        user_temp = user_temp.sort_values(by='min_centroids')\n",
    "        user_temp['artist_rank'] = user_temp['min_centroids'].rank(method = 'first', ascending=True) + len(user_rank_artist)\n",
    "        user_rank_artist = pd.concat([user_rank_artist,user_temp])\n",
    "\n",
    "\n",
    "    # in original ranking, drop tracks already reranked by artist frequency for artists identified in df_user_apparent\n",
    "    initial_ranking = initial_ranking.drop(user_rank_artist.index)\n",
    "\n",
    "    # sort values of initial ranking by minimum found for each centroid\n",
    "    initial_ranking['min_centroids'] = initial_ranking.iloc[:,2:-2].min(axis=1)\n",
    "    initial_ranking = initial_ranking.sort_values(by='min_centroids')\n",
    "    # attribute them new ranks based on the length of user_rank_artist to get a single rank by track/artist\n",
    "    initial_ranking['artist_rank'] = initial_ranking['min_centroids'].rank(method='first') + len(user_rank_artist)   \n",
    "\n",
    "    # combine new ranking based on artists identified and initial ranks reranked as a continuity of artist based ranks\n",
    "    full_ranks = pd.concat([user_rank_artist, initial_ranking])\n",
    "    full_ranks['best_rank'] = full_ranks.iloc[:, -2:].min(axis=1)\n",
    "\n",
    "    return full_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_clusters(df:pd.DataFrame, drop : list[str], user_id: str):\n",
    "    \n",
    "    # definition of df_user\n",
    "    df_user = df[df['user']==user_id].drop(drop, axis=1).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "    # weights to apply to the kmeans\n",
    "    user_listenings = df_user['listening_count']\n",
    "\n",
    "    # eliminate weights from acoustic characteristics\n",
    "    df_user = df_user.drop('listening_count', axis=1) \n",
    "    \n",
    "\n",
    "    # broad exploration of number of clusters\n",
    "    dist = []\n",
    "    s_scores = []\n",
    "    range = np.arange(2,len(df_user)-1)\n",
    "    for i in range:\n",
    "        clf = KMeans(n_clusters = i, n_init = 'auto', random_state=123)\n",
    "        clf.fit(df_user, sample_weight=user_listenings)\n",
    "        centroids = clf.cluster_centers_\n",
    "        labels = clf.labels_\n",
    "        score = silhouette_score(df_user, labels, metric='euclidean')\n",
    "        s_scores.append(score)\n",
    "        dist.append(sum(np.min(cdist(XA = centroids, XB = df_user, metric='euclidean'), axis = 0)) / len(df_user))\n",
    "    \n",
    "    # optimal number of clusters selection by silhouette score\n",
    "    n_clusters = s_scores.index(max(s_scores))+2\n",
    "\n",
    "    return range, s_scores, dist, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_profiles(df: pd.DataFrame, drop : list[str],  user_id: str, n_clusters:int):\n",
    "    \"\"\"\n",
    "    Takes a user_id string and 1 dataframe containg all original informations and whose acoustic characteristics are Standard scaled.\n",
    "    Returns the acoustic characteristics of the user standard profile weighted by number of listenings (1 acoustic profile)   \n",
    "    and the cluster centers centroids profiles based on Kmeans fit on optimal number of cluster found per user (n_clusters profiles)\n",
    "\n",
    "    Parameters:\n",
    "        df : whole dataframe with all acoustic characteristics Standard scaled\n",
    "        user_id : str corresponding to the user id\n",
    "   \n",
    "    Return :\n",
    "        \n",
    "        standard_profile: list[list[float]] of size 1\n",
    "        cluster_profile: list[list[float]] of size n_clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    total_listening = df[df[\"user\"] == user_id][\"listening_count\"].sum() \n",
    "\n",
    "    ## Standard profile: one profile where acoustic characteristics are weighted by listening_count\n",
    "    '''\n",
    "    hotness = 0\n",
    "    familiarity = 0\n",
    "    '''\n",
    "    duration = 0\n",
    "    dance = 0\n",
    "    energy = 0\n",
    "    loudness = 0\n",
    "    speech = 0\n",
    "    acoustic = 0\n",
    "    instru = 0\n",
    "    live = 0\n",
    "    valence = 0\n",
    "    tempo = 0\n",
    "    \n",
    "    # cumulative sum of user's  songs acoustic characteristics weighted by listening_count\n",
    "    for index, row, in df[df[\"user\"] == user_id].iterrows():\n",
    "        '''\n",
    "        hotness += row['artist_hotttnesss'] * row[\"listening_count\"]\n",
    "        familiarity += row['artist_familiarity'] * row[\"listening_count\"]   \n",
    "        '''\n",
    "        #duration += row['duration'] * row[\"listening_count\"]\n",
    "        dance += row['danceability'] * row[\"listening_count\"]\n",
    "        energy += row['energy'] * row[\"listening_count\"]\n",
    "        loudness += row['loudness'] * row[\"listening_count\"]\n",
    "        speech += row['speechiness'] * row[\"listening_count\"]\n",
    "        acoustic += row['acousticness'] * row[\"listening_count\"]\n",
    "        instru += row['instrumentalness'] * row[\"listening_count\"]\n",
    "        live += row['liveness'] * row[\"listening_count\"]\n",
    "        valence += row['valence'] * row[\"listening_count\"]\n",
    "        tempo += row['tempo'] * row[\"listening_count\"]\n",
    "\n",
    "    # duration / total_listening, \n",
    "\n",
    "    # weighted average from cumulative sum of acoustic characteristics / total_listening to list of lists\n",
    "    standard_profile = [[dance / total_listening, energy / total_listening,\n",
    "                        loudness / total_listening, speech / total_listening, acoustic / total_listening,\n",
    "                        instru / total_listening, live / total_listening, valence/total_listening, tempo / total_listening]]\n",
    "\n",
    "    ## Cluster profile\n",
    "    \n",
    "    # definition of df_user\n",
    "    df_user = df[df['user']==user_id].drop(drop, axis=1).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "    # weights to apply to the kmeans\n",
    "    user_listenings = df_user['listening_count']\n",
    "\n",
    "    # eliminate weights from acoustic characteristics\n",
    "    df_user = df_user.drop('listening_count', axis=1) \n",
    "\n",
    "    # training of kmeans with optimal number of clusters\n",
    "    clf = KMeans(n_clusters = n_clusters, n_init = 'auto', random_state=123)\n",
    "    clf.fit(df_user, sample_weight=user_listenings)\n",
    "    centroids = clf.cluster_centers_\n",
    "    labels = clf.labels_\n",
    "\n",
    "    # distinct profiles as cluster centers centroids to list\n",
    "    cluster_profile = centroids.tolist()\n",
    "\n",
    "    \n",
    "    return standard_profile, cluster_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_neighbors(df_features, df_features_names: pd.DataFrame, profiles: list[list[float]], n_target : int):\n",
    "    \n",
    "    \n",
    "    user_profile = pd.DataFrame(columns=[\"danceability\", \"energy\",\n",
    "                                   \"loudness\", \"speechiness\", \"acousticness\",\n",
    "                                   \"instrumentalness\", \"liveness\", \"valence\", \n",
    "                                   \"tempo\"])\n",
    "\n",
    "    # append profile to the dataframe user_profile\n",
    "    for i in range(len(profiles)):\n",
    "        user_profile.loc[len(user_profile)] = profiles[i]\n",
    "\n",
    "\n",
    "    # compute the distance matrix between all user_profiles and all songs features\n",
    "    dist_mat = pd.DataFrame(distance_matrix(user_profile, df_features).T)\n",
    "    #dist_mat = dist_mat.rename(columns={0 : 'distance'})\n",
    "    \n",
    "\n",
    "    # create similarity and disimilarity lists\n",
    "    sim = []\n",
    "    dis = []\n",
    "    sim_id = []\n",
    "    dis_id =[]\n",
    "\n",
    "    # broadcast distance of matrix[i] to df_features_names containing song and artist names and sort values by distance[i]\n",
    "    for i in range(len(profiles)):\n",
    "        df_feats_names = df_features_names\n",
    "        df_feats_names[i] = dist_mat[i]\n",
    "        df_feats_names = df_feats_names.sort_values(by=i, ascending=True).reset_index().drop(['index'], axis=1)\n",
    "        \n",
    "        # for each profile create temporary sim and disim lists\n",
    "        s = []\n",
    "        d = []\n",
    "        s_id = []\n",
    "        d_id = []\n",
    "\n",
    "        ## append to each profile list the closest and furthest n_neighbors tracks and corresponding song-artist names \n",
    "        for j in range (n_target):\n",
    "            \n",
    "            \n",
    "            s.append(df_feats_names.iloc[j, :9].values.flatten().tolist())\n",
    "            s_id.append(' - '.join((df_feats_names.iloc[j, 9:11])))\n",
    "            \n",
    "            d.append(df_feats_names.iloc[len(df_feats_names)-1-j, :9].values.flatten().tolist())\n",
    "            d_id.append(' - '.join((df_feats_names.iloc[len(df_feats_names)-1-j, 9:11])))\n",
    "            \n",
    "            '''\n",
    "            s.append(df_feats_names.iloc[j, :8].values.flatten().tolist())\n",
    "            s_id.append(' - '.join((df_feats_names.iloc[j, 8:10])))\n",
    "            \n",
    "            d.append(df_feats_names.iloc[len(df_feats_names)-1-j, :8].values.flatten().tolist())\n",
    "            d_id.append(' - '.join((df_feats_names.iloc[len(df_feats_names)-1-j, 8:10])))\n",
    "            '''\n",
    "        sim.append(s)\n",
    "        sim_id.append(s_id)\n",
    "        dis.append(d)\n",
    "        dis_id.append(d_id)\n",
    "\n",
    "    return sim, dis, sim_id, dis_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548e880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
