{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b8cd1-63e1-48d1-8870-c98fcee7b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451a6d1-f6fa-47c9-99c1-c226ab4559ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942111bb-2d23-49c1-9e98-a79122be56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listenings_history_partition(listenings_history_feats, max_clusters = 10, random_state = None):\n",
    "    s_scores = []\n",
    "    labels = []\n",
    "    centroids = []\n",
    "    n_clusters_range = np.arange(2, min(max_clusters, len(listenings_history_feats) - 1))\n",
    "    \n",
    "    # Search the optimal number of clusters\n",
    "    for i in n_clusters_range:\n",
    "        clf = KMeans(n_clusters = i, n_init = 'auto', random_state = random_state)\n",
    "        clf.fit(listenings_history_feats)\n",
    "        centroids.append(clf.cluster_centers_)\n",
    "        labels.append(clf.labels_)\n",
    "        score = silhouette_score(listenings_history_feats, labels[i-2], metric='euclidean')\n",
    "        s_scores.append(score)\n",
    "    \n",
    "    # Define the optimal number of clusters from the silhouette score\n",
    "    i_clusters_opt = s_scores.index(max(s_scores))\n",
    "    \n",
    "    # Return the corresponding partition, \n",
    "    return labels[i_clusters_opt], centroids[i_clusters_opt], s_scores[i_clusters_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdec73-d19d-443e-8635-34ab5696e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_based_knn(listenings_history, X, n_neighbors, weighted = True, n_clusters = 'auto', random_state = None):  \n",
    "    # Get the features of the songs listened by the user\n",
    "    listenings_history_feats = X.loc[listenings_history.track_id]      \n",
    "    \n",
    "    # If a weighting of each tracks is desired :\n",
    "    if weighted:\n",
    "        # Calculation of the weight of each track according to its number of listens\n",
    "        w = listenings_history.listening_count / listenings_history.listening_count.sum()\n",
    "        w.index = listenings_history_feats.index \n",
    "        # Tracks weighting\n",
    "        listenings_history_feats = listenings_history_feats.apply(lambda x: x*w)\n",
    "    \n",
    "    # Compute the user tracks centroids\n",
    "    if n_clusters == 'auto':\n",
    "        labels, centroids, _ = get_listenings_history_partition(listenings_history_feats)\n",
    "    elif n_clusters == 1:\n",
    "        labels = np.zeros(len(listenings_history))\n",
    "        centroids = listenings_history_feats.apply('mean')\n",
    "    else:\n",
    "        clf = KMeans(n_clusters = n_clusters, n_init = 'auto', random_state = random_state)\n",
    "        clf.fit(listenings_history_feats)\n",
    "        centroids = clf.cluster_centers_\n",
    "        labels = clf.labels_\n",
    "    \n",
    "    # Define the number of neighbors to find according to the clusters size\n",
    "    cluster_size = pd.Series(labels).value_counts()\n",
    "    nb_tracks_by_clusters = [round(n_neighbors * v) for v in cluster_size / sum(cluster_size)]\n",
    "    if not sum(nb_tracks_by_clusters) == n_neighbors:\n",
    "        nb_tracks_by_clusters[-1] = n_neighbors - sum(nb_tracks_by_clusters[:-1])\n",
    "        \n",
    "    # Compute the distnce between the tracks and the centroids  \n",
    "    D = pd.DataFrame(distance_matrix(X.loc[X.index.difference(listenings_history_feats.index)], centroids), index = X.index.difference(listenings_history_feats.index))           \n",
    "\n",
    "    # Get the ranks of the tracks relating to its distance with each centroid\n",
    "    R = D.rank(axis = 0)\n",
    "\n",
    "    # Get the n_neighbors unique recommended tracks\n",
    "    recommended_tracks = []\n",
    "    for i, n in enumerate(nb_tracks_by_clusters):\n",
    "        tracks = [t for t in list(R.iloc[:,i].sort_values().index) if not t in recommended_tracks]\n",
    "        recommended_tracks = recommended_tracks + tracks[:n]\n",
    "\n",
    "    return(recommended_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0274c7d-a8eb-4531-bf6d-cee8732b3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_based_ranking(listenings_history, X, weighted = True, n_clusters = 'auto', random_state = None):  \n",
    "    \"\"\"\n",
    "    Define the centroid(s) representing the average audio features of the tracks listened by a user.\n",
    "    Estimate the affinity of the user for tracks that he didn't listened based on their distance with this (these) centroid(s).\n",
    "    Return the rank of the unlistened tracks based on this afinity.\n",
    "    \n",
    "    Parameters:\n",
    "        listening_history (pandas.Dataframe): a dataframe whose columns are\n",
    "            - user : a unique id of the user\n",
    "            \n",
    "            - track_id : a unique id for a track\n",
    "            \n",
    "            - listening_count: the number of times the user has listened to the track\n",
    "            \n",
    "            ... : other columns corresponding to track's features and/or the user's features and/or element of context of the interaction user/track.\n",
    "        \n",
    "        X (pandas.Dataframe): a dataframe corresponding to the audio_features of the tracks.\n",
    "        \n",
    "        weighted (boolean, default = True): determines whether the centroid calculation is weighted by the listens number of the tracks.    \n",
    "         \n",
    "        n_clusters (int or 'auto'): define the number of centroids to form. If 'auto', it is determine as the arg max of the silhouette score.\n",
    "        \n",
    "        \n",
    "        random_state (int, default=None): pass an int for reproducible output across multiple function calls.\n",
    "        \n",
    "    Return :\n",
    "       R : the ranks of each tracks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the features of the songs listened by the user\n",
    "    listenings_history_feats = X.loc[listenings_history.track_id]      \n",
    "    \n",
    "    # If a weighting of each tracks is desired :\n",
    "    if weighted:\n",
    "        # Calculation of the weight of each track according to its number of listens\n",
    "        w = listenings_history.listening_count / listenings_history.listening_count.sum()\n",
    "        w.index = listenings_history_feats.index \n",
    "        # Tracks weighting\n",
    "        listenings_history_feats = listenings_history_feats.apply(lambda x: x*w)\n",
    "    \n",
    "    # Compute the user tracks centroids\n",
    "    if n_clusters == 'auto':\n",
    "        labels, centroids, _ = get_listenings_history_partition(listenings_history_feats)\n",
    "    elif n_clusters == 1:\n",
    "        labels = np.zeros(len(listenings_history))\n",
    "        centroids = listenings_history_feats.apply('mean')\n",
    "    else:\n",
    "        clf = KMeans(n_clusters = n_clusters, n_init = 'auto', random_state = random_state)\n",
    "        clf.fit(listenings_history_feats)\n",
    "        centroids = clf.cluster_centers_\n",
    "        labels = clf.labels_\n",
    "    \n",
    "    # Define the number of neighbors to find according to the clusters size\n",
    "    # cluster_size = pd.Series(labels).value_counts()\n",
    "    # nb_tracks_by_clusters = [round(n_neighbors * v) for v in cluster_size / sum(cluster_size)]\n",
    "    # if not sum(nb_tracks_by_clusters) == n_neighbors:\n",
    "    #     nb_tracks_by_clusters[-1] = n_neighbors - sum(nb_tracks_by_clusters[:-1])\n",
    "        \n",
    "    # Compute the distnce between the tracks and the centroids  \n",
    "    D = pd.DataFrame(distance_matrix(X.loc[X.index.difference(listenings_history_feats.index)], centroids), index = X.index.difference(listenings_history_feats.index))           \n",
    "\n",
    "#     # Get the ranks of the tracks relating to its distance with each centroid\n",
    "    R = D.rank(axis = 0)\n",
    "\n",
    "#     # Get the n_neighbors unique recommended tracks\n",
    "#     recommended_tracks = []\n",
    "#     for i, n in enumerate(nb_tracks_by_clusters):\n",
    "#         tracks = [t for t in list(R.iloc[:,i].sort_values().index) if not t in recommended_tracks]\n",
    "#         recommended_tracks = recommended_tracks + tracks[:n]\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5b3fe-e51e-4bd8-a81a-940fdc941d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045eedaa-8d47-423a-b45d-7a28396bb342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
